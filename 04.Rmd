# Time series features

```{r 4libraries, echo = FALSE}
suppressMessages(library(tidyverse))
library(fpp3)
```

**Learning objectives:**

- Review simple time series features (i.e., mean, quantile).
 
- Discuss the concept of ACF (Autocorrelation Features) and lag.
 
- Discuss the concept of STL (Seasonal, Trend, Lag) decomposition.
 
- Introduce other features from `feast` package.

## 4.1 - Some simple statistics

- Time series numerical summary (ex. mean, median, minimum and maximum values) can be computed using the `features()` function.

- Example: Compute the means of all the series in the Australian tourism data.
```{r 4ts_means}
data("tourism")

tourism %>% 
     features(Trips, list(mean = mean)) %>% 
     arrange(mean)
```

- The least average number of visits corresponds to "Other" purpose category.

## Five summary statistics

- Five summary statistics (minimum - 0%, first quantile - 25%, median - 50% quantile, third quantile - 75% and maximum values - 100%) for each series.
```{r 4summary_statistics}
tourism %>% 
     features(Trips, quantile)
```

## 4.2 ACF features

- We can also summarise the autocorrelations to produce new features; for example, the sum of the first ten squared autocorrelation coefficients is a useful summary of how much autocorrelation there is in a series, regardless of lag.

- The `feat_acf()` function computes a selection of the autocorrelations discussed here. It will return six or seven features:

     - the first autocorrelation coefficient from the original data;
    
     - the sum of squares of the first ten autocorrelation coefficients from the original data;
    
     - the first autocorrelation coefficient from the differenced data;
    
     - the sum of squares of the first ten autocorrelation coefficients from the differenced data;
    
     - the first autocorrelation coefficient from the twice differenced data;
    
     - the sum of squares of the first ten autocorrelation coefficients from the twice differenced data;
    
     - For seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned.

## 

When applied to the Australian tourism data, we get the following output.
```{r 4feat_acf}
tourism %>% 
     features(Trips, feat_acf)
```

## 4.3 - STL features

A time series decomposition can be used to measure the strength of trend and seasonality in a time series. Recall that the decomposition is written as:

<div align = "center">
$y_{t} = T_{t} + S_{t} + R_{t}$,
</div>

where $T_{t}$ is the smoothed trend component, $S_{t}$ is the seasonal component and $R_{t}$ is a remainder component.

- These measures can be useful, for example, when you have a large collection of time series, and you need to find the series with the most trend or the most seasonality.

- These and other STL-based features are computed using the `feat_stl()` function.
```{r 4feat_stl}
tourism %>% 
     features(Trips, feat_stl)
```

## 

We can then use these features in plots to identify what type of series are heavily trended and what are most seasonal.
```{r 4feat_stl_plot_1}
tourism %>%
     features(Trips, feat_stl) %>%
     ggplot(aes(x = trend_strength, y = seasonal_strength_year,
                col = Purpose)) +
     geom_point() +
     facet_wrap(vars(State))
```

- Clearly, holiday series are most seasonal, which is not surprising.

- The strongest trends tend to be in Western Australia and Victoria.

## 

The most seasonal series can also be easily identified and plotted.
```{r 4feat_stl_plot_2}
tourism %>%
     features(Trips, feat_stl) %>%
     filter(
          seasonal_strength_year == max(seasonal_strength_year)
     ) %>%
     left_join(tourism, by = c("State", "Region", "Purpose")) %>%
     ggplot(aes(x = Quarter, y = Trips)) +
     geom_line() +
     facet_grid(vars(State, Region, Purpose))
```

The above plot shows holiday trips to the most popular ski region of Australia.

## 

The feat_stl() function returns several more features other than those discussed above.

    - `seasonal_peak_year` indicates the timing of the peaks — which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
    
    - `seasonal_trough_year` indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.
    
    - `spikiness` measures the prevalence of spikes in the remainder component Rt of the STL decomposition. It is the variance of the leave-one-out variances of Rt.
    
    - `linearity` measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
    
    - `curvature` measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
    
    - `stl_e_acf1` is the first autocorrelation coefficient of the remainder series.
    
    - `stl_e_acf10` is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

## 4.4 - Other features

Many more features are possible, and the `feasts` package computes only a few dozen features that have proven useful in time series analysis. It is also easy to add your own features by writing an R function that takes a univariate time series input and returns a numerical vector containing the feature values.

Refer to the textbook for a detailed explanation on the reamining features generated by the `feasts` package.

## 4.5 - Exploring Australian tourism data

All of the features included in the `feasts` package can be computed in one line like this.
```{r feasts_all}
tourism_features <- tourism %>% 
     features(Trips, feature_set(pkgs = "feasts"))

tourism_features
```

This gives **48 features** for every combination of the three key variables (Region, State and Purpose). We can treat this tibble like any data set and analyse it to find interesting observations or groups of observations.

## 

We can also do pairwise plots of groups of features (Figure 4.3).
```{r 4ggally}
library(glue)
tourism_features %>%
     select_at(vars(contains("season"), Purpose)) %>%
     mutate(
          seasonal_peak_year = seasonal_peak_year +
               4*(seasonal_peak_year==0),
          seasonal_trough_year = seasonal_trough_year +
               4*(seasonal_trough_year==0),
          seasonal_peak_year = glue("Q{seasonal_peak_year}"),
          seasonal_trough_year = glue("Q{seasonal_trough_year}"),
     ) %>%
     GGally::ggpairs(mapping = aes(colour = Purpose))
```

Here, the Purpose variable is mapped to colour. There is a lot of information in this figure, and we will highlight just a few things we can learn.

    - The three numerical measures related to seasonality (seasonal_strength_year, season_acf1 and season_pacf) are all positively correlated.
    
    - The bottom left panel and the top right panel both show that the most strongly seasonal series are related to holidays (as we saw previously).
    
    - The bar plots in the bottom row of the seasonal_peak_year and seasonal_trough_year columns show that seasonal peaks in Business travel occurs most often in Quarter 3, and least often in Quarter 1.

## Principal Component Analysis (PCA)

- A useful way to handle many more variables is to use a dimension reduction technique such as principal components.

- This gives linear combinations of variables that explain the most variation in the original data.

- We can compute the principal components of the tourism features as follows.
```{r 4pca}
library(broom)

pcs <- tourism_features %>%
     select(-State, -Region, -Purpose) %>%
     prcomp(scale = TRUE) %>%
     augment(tourism_features)

pcs %>%
     ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
     geom_point() +
     theme(aspect.ratio = 1)
```

- Each point on Figure 4.4 represents one series and its location on the plot is based on all 48 features.

- The first principal component (`.fittedPC1`) is the linear combination of the features which explains the most variation in the data.

- The second principal component (`.fittedPC2`) is the linear combination which explains the next most variation in the data, while being uncorrelated with the first principal component.

- For more information about principal component dimension reduction, see Izenman (2008).

## 

- The preceding plot also allows us to identify anomalous time series — series which have unusual feature combinations.

- These appear as points that are separate from the majority of series in Figure 4.4. There are four that stand out, and we can identify which series they correspond to as follows.
```{r 4outliers}
outliers <- pcs %>% 
     filter(.fittedPC1 > 10) %>% 
     select(Region, State, Purpose, .fittedPC1, .fittedPC2)

outliers
```

```{r outliers_plot}
outliers %>%
     left_join(tourism, by = c("State", "Region", "Purpose")) %>%
     mutate(
          Series = glue("{State}", "{Region}", "{Purpose}",
                        .sep = "\n\n")
     ) %>%
     ggplot(aes(x = Quarter, y = Trips)) +
     geom_line() +
     facet_grid(Series ~ ., scales = "free") +
     labs(title = "Outlying time series in PC space")
```

##

We can speculate why these series are identified as unusual.

    - Holiday visits to the south coast of NSW is highly seasonal but has almost no trend, whereas most holiday destinations in Australia show some trend over time.
    
    - Melbourne is an unusual holiday destination because it has almost no seasonality, whereas most holiday destinations in Australia have highly seasonal tourism.
    
    - The north western corner of Western Australia is unusual because it shows an increase in business tourism in the last few years of data, but little or no seasonality.
    
    - The south western corner of Western Australia is unusual because it shows both an increase in holiday tourism in the last few years of data and a high level of seasonality.

Source: Izenman, A. J. (2008). Modern multivariate statistical techniques: Regression, classification and manifold learning. Springer.

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/J2YWrDR3fF8")`

<details>
<summary> Meeting chat log </summary>

```
LOG
```
</details>
